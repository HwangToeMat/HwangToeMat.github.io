---
title: 1. MTCNN 논문 리뷰
image: https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img0.png?raw=true
description: >
 MTCNN - Joint Face Detection and Alignment using Multi task Cascaded Convolutional Networks을 읽고 논문 주요내용을 정리해본다.
author: author1
layout: post
order: 15
---
# [Face Recognition] 1. MTCNN 논문 리뷰

<a href="https://arxiv.org/abs/1604.02878.pdf">[PDF] 논문원본</a>

# MTCNN: Joint Face Detection and Alignment using Multi task Cascaded Convolutional Networks

## 모델 구조

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img1.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

MTCNN은 3개의 neural network(P-Net, R-Net, O-Net)로 이루어져 있다. 
이때 각 net에서 face detection과 bbox regression, face alignment를 단계적으로 진행하면서 동시에 학습시키는 방식(joint learning)을 사용한다. 
joint learning을 통해 기존의 다른 방법에 비해 정확도가 높고 속도가 빠르며 그렇기 때문에 지금까지도 많이 쓰이고 있는 논문이다. 

MTCNN의 구조를 단계별로 살펴보면 아래와 같이 총 4단계로 이루어진다.

### 0단계. Image Pyramid

### 1단계. P-Net(Proposal Network)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img2.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

### 2단계. R-Net(Refine Network)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img3.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">


### 3단계. O-Net(Output Network)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img4.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">


## 여러 경우에서의 실험

* Depthwise Separable vs Full Convolution MobileNet(일반적인 Conv)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img4_1.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

먼저 일반적인 3x3 Conv를 사용한 모델과 이 논문에서 제안한 방법(Depthwise Separable)을 사용한 모델간의 비교이다. **정확도는 1% 정도 밖에 차이**가 나지 않는 것에 비해 **연산량과 파라미터 수는 압도적으로 줄어든 것**을 볼 수 있다.

* MobileNet Width Multiplier

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img4_3.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

Width Multiplier는 **모델 내부의 각 layer의 width를 &#945; 만큼의 비율로 줄여** 연산량과 파라미터 수를 줄이는 방법이다. 위의 차트에서 MobileNet 앞의 숫자들(1.0, 0.75, 0.5, 0.25)이 그것이다. &#945;의 값이 클수록 모델은 가벼워지지만 정확도가 낮아진다. 특히 0.5 아래부터는 정확도가 큰폭으로 낮아지는 것을 알 수 있다. 

* MobileNet Resolution Multiplier

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img4_4.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

Resolution Multiplier는 **모델의 input 이미지의 resolution을 &#961;로 변경 하여** 연산량을 변경시키는 방법이다. 위의 차트에서 MobileNet 뒤의 숫자들(224, 192, 160, 128)이 그것이다. &#961;의 값이 작을수록 모델은 연산량이 줄어들지만 정확도가 낮아진다. input값이 달라진것이기 때문에 파라미터 수는 달라지지 않는다.

## 다양한 task에서 성능 비교

* Classification에서 성능 비교

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img5.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

* Object Detection에서 성능 비교

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img6.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

* Face Recognition에서 성능 비교

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img7.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">
