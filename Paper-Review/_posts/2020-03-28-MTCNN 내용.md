---
title: 1. MTCNN 논문 리뷰
image: https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img0.png?raw=true
description: >
 MTCNN - Joint Face Detection and Alignment using Multi task Cascaded Convolutional Networks을 읽고 논문 주요내용을 정리해본다.
author: author1
layout: post
order: 15
---
# [Face Recognition] 1. MTCNN 논문 리뷰

<a href="https://arxiv.org/abs/1604.02878.pdf">[PDF] 논문원본</a>

# MTCNN: Joint Face Detection and Alignment using Multi task Cascaded Convolutional Networks

## 모델 구조

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img1.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

MTCNN은 **3개의 neural network(P-Net, R-Net, O-Net)로** 이루어져 있다. 이때 각 net에서 face classification과 bbox regression, face landmark localization 과정을 진행하면서 동시에 학습시키는 방식(**joint learning**)을 사용한다. joint learning을 통해 기존의 다른 방법에 비해 정확도가 높고 속도가 빠르며 그렇기 때문에 지금까지도 많이 쓰이고 있는 논문이다. 

MTCNN의 구조를 단계별로 살펴보면 아래와 같이 **총 4단계**로 이루어진다.

### 0단계. Image Pyramid

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img1_1.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

본격적으로 detection이 이루어지기 전에 위와같이 **input되는 이미지를 각기 다른 scale로 resize하여 image pyramid를 만든다.** 이는 앞서 살펴보았던 SSD에서와 같이 다양한 scale의 이미지로 **다양한 사이즈의 얼굴을 더 잘 detection**하기 위해서 라고 볼 수 있다.

### 1단계. P-Net(Proposal Network)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img2.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

첫번째로 **P-Net은 이미지에서 얼굴을 찾아내는데 중점을 둔 network이다.** P-Net은 이 논문에서 'We exploit a fully convolutional network[?]'이라고 말했듯이 fully connected layer가 없고 conv layer로 이루어진 것이 특징이다. 

이는 [“Multi-view face detection using deep convolutional neural networks,” in ACM on International Conference on Multimedia Retrieval, 2015, pp. 643-650.]에서 bbox regression vector와 후보 영역을 구하는 방법과 유사하다. 

P-Net을 통해 수 많은 bbox regression vector와 후보 영역을 얻게되는데 이들을 **NMS(non-maximum suppression)알고리즘으로 높은 정확도의 후보 영역만 남도록 추려낸다.** 그리고 남은 후보영역의 face classification, bbox regression, face landmark localization 값을 다음 단계로 보낸다.

### 2단계. R-Net(Refine Network)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img3.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

두번째로 **R-Net은 P-Net에서 찾아낸 후보 영역을 추려내는데 중점을 둔 network이다.** R-Net의 구조는 P-Net의 구조와 유사하지만 fully connected layer가 끝에 추가됬다는 점이 특징이다. 

R-Net에서도 bbox regression vector와 후보 영역을 얻어내고, 이들을 NMS(non-maximum suppression)알고리즘으로 높은 정확도의 후보 영역만 남도록 추려낸다. 이때 fully connected layer가 있기때문에 조금더 정확한 값을 추려낼 수 있다고 추측할 수 있다. 마찬가지로 남아있는 후보영역의 face classification, bbox regression, face landmark localization 값을 다음 단계로 보낸다.

### 3단계. O-Net(Output Network)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img4.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

세번째로 **O-Net은 이전 단계에서 찾아낸 후보 영역에서 face landmark를 찾아내는데 중점을 둔 network이다.**

이를 통해 **최종적인 face classification, bbox regression, face landmark localization값을 얻게 된다.**

이때 각 단계에서 나오는 값들을 자세히 살펴보면 아래와 같다.
> * face classification (2개)
> y<sup>det</sup> = GT에서 얼굴이 있는지 여부(있을때 1, 없을때 0)
> p = 얼굴이 있을 확률
> * bbox regression (4개)
> 예측한 bbox의 왼쪽상단 x,y좌표
> 예측한 bbox의 너비와 높이
> * face landmark localization (10갸)
> 왼쪽 눈의 x,y 좌표
> 오른쪽 눈의 x,y 좌표
> 코의 x,y 좌표
> 입의 왼쪽 끝 부분의 x,y 좌표
> 입의 오른쪽 끝 부분의 x,y 좌표

## 여러 경우에서의 실험

* Depthwise Separable vs Full Convolution MobileNet(일반적인 Conv)

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MTCNN/img4_1.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

먼저 일반적인 3x3 Conv를 사용한 모델과 이 논문에서 제안한 방법(Depthwise Separable)을 사용한 모델간의 비교이다. **정확도는 1% 정도 밖에 차이**가 나지 않는 것에 비해 **연산량과 파라미터 수는 압도적으로 줄어든 것**을 볼 수 있다.

* MobileNet Width Multiplier

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img4_3.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

Width Multiplier는 **모델 내부의 각 layer의 width를 &#945; 만큼의 비율로 줄여** 연산량과 파라미터 수를 줄이는 방법이다. 위의 차트에서 MobileNet 앞의 숫자들(1.0, 0.75, 0.5, 0.25)이 그것이다. &#945;의 값이 클수록 모델은 가벼워지지만 정확도가 낮아진다. 특히 0.5 아래부터는 정확도가 큰폭으로 낮아지는 것을 알 수 있다. 

* MobileNet Resolution Multiplier

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img4_4.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

Resolution Multiplier는 **모델의 input 이미지의 resolution을 &#961;로 변경 하여** 연산량을 변경시키는 방법이다. 위의 차트에서 MobileNet 뒤의 숫자들(224, 192, 160, 128)이 그것이다. &#961;의 값이 작을수록 모델은 연산량이 줄어들지만 정확도가 낮아진다. input값이 달라진것이기 때문에 파라미터 수는 달라지지 않는다.

## 다양한 task에서 성능 비교

* Classification에서 성능 비교

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img5.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

* Object Detection에서 성능 비교

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img6.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

* Face Recognition에서 성능 비교

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/Paper-Review/image/MobileNets/img7.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">
