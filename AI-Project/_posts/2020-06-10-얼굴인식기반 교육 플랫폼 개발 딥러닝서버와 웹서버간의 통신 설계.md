---
title: 4-3 얼굴인식기반 교육 플랫폼 개발 - 딥러닝서버와 웹서버간의 통신 설계
image: https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/AI-Project/image/KHUFACE/3/img0.png?raw=true
description: >
 얼굴인식기반 교육 플랫폼 개발과정 중 백엔드 및 클라우드 설정한다.
author: author1
layout: post
order: 8
---

# 4-3 얼굴인식기반 교육 플랫폼 개발[KHUFACE ID] - 딥러닝서버와 웹서버간의 통신 설계

<a href="https://github.com/HwangToeMat/FaceRecognition_FlaskServer">[Github] 코드 원본</a>

## 딥러닝 서버와 웹 서버간의 통신

딥러닝 서버와 웹 서버는 Rest API방식으로 통신한다. 그 중 GET을 사용하여 영상을 제공받는 카메라의 IP를 웹으로 부터 받고, 그 영상을 분석하여 얻은 정보를 웹으로 다시 보내주는 방식을 사용한다. 이때 사용되는 3가지 함수는 아래와 같다.

### 얼굴위치 확인 함수 - Modelfr

- Code

```python
def get_face(URL, device, targets=target, names=name):
    student_list = []
    frame = URL2Frame(URL)
    try:
        bboxes, landmarks = MTCNN_NET(frame, 0.5, device, 'MTCNN/weights/pnet_Weights',
                                      'MTCNN/weights/rnet_Weights', 'MTCNN/weights/onet_Weights')
        landmarks = landmarks.tolist()
        for i, b in enumerate(bboxes):
            box = dict()
            box['x1'] = b[0]
            box['y1'] = b[1]
            box['x2'] = b[2]
            box['y2'] = b[3]
            box['landmarks'] = landmarks[i]
            student_list.append(box)
        return student_list
    except:
        return []
```

- input

```json
ip = http://xxx.xxx.xxx.xxx:xxxx/shot.jpg # ip camera의 영상 주소
landmark = [xxxx, xxxx, xxxx.....] # modelfr에서 찾은 얼굴의 landmark
```

- ouput

```json
{"box":[{"landmarks":[xxxx, xxxx, xxxx.....],"x1":xxx,"x2":xxx,"y1":xxx,"y2":xxx},
        {"landmarks":[yyyy, yyyy, yyyy.....],"x1":yyy,"x2":yyy,"y1":yyy,"y2":yyy}]} 
        # 인식한 학생의 얼굴에서 찾은 landmark와 bounding box의 
```

- 실행화면

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/AI-Project/image/KHUFACE/2/img1.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

**Modelfr은 수업시간 중 웹에서 계속해서 요청을 하는 함수이며 얼굴의 위치를 파악하여 웹으로 실시간 전송한다. 그리고 웹에서는 이러한 정보를 바탕으로 얼굴의 위치에 맞는 버튼을 실시간으로 생성힌다.**

### 얼굴 정보 확인 함수 - Modelin

- Code

```python
def get_id(URL, device, targets=target, names=name):
    student_list = []
    frame = URL2Frame(URL)
    try:
        faces = Face_alignment(
            frame, default_square=True, landmarks=landmarks)

        embs = []

        test_transform = trans.Compose([
            trans.ToTensor(),
            trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

        for img in faces:
            embs.append(detect_model(
                test_transform(img).to(device).unsqueeze(0)))

        if embs != []:
            source_embs = torch.cat(embs)
            diff = source_embs.unsqueeze(-1) - \
                targets.transpose(1, 0).unsqueeze(0)
            dist = torch.sum(torch.pow(diff, 2), dim=1)
            minimum, min_idx = torch.min(dist, dim=1)
            min_idx[minimum > ((75-156)/(-80))] = -1
            results = min_idx

            for i, k in enumerate(bboxes):
                if results[i] == -1:
                    continue
                student_list.append(names[results[i] + 1])
        return student_list
    except:
        return []
```

- input

```json
ip = http://xxx.xxx.xxx.xxx:xxxx/shot.jpg # ip camera의 영상 주소
landmark = [xxxx, xxxx, xxxx.....] # modelfr에서 찾은 얼굴의 landmark
```

- ouput

```json
{"id":["0"]} # 인식한 학생의 id 값
```

- 실행화면

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/AI-Project/image/KHUFACE/2/img2.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

**Modelin은 웹에 생성된 버튼을 클릭할 시 해당 얼굴의 특징과 DB의 정보를 비교하여 신원을 확인하여 웹으로 g2.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">

121



122

**Modelin은 웹에 생성된 버튼을 클릭할 시 해당 얼굴의 특징과 DB의 정보를 비교하여 신원을 확인하여 웹으로 전송한다. 이를 통해 웹에서는 해당 인원에 대한 정보를 가져와 사용자가 쉽게 확인 및 수정 할 수 있도록 화면을 제공한다.**

### 출결 정보 확인 함수 - Modelat

- Code

```python
def get_id(URL, device, targets=target, names=name):
    student_list = []
    frame = URL2Frame(URL)
    try:
        bboxes, landmarks = MTCNN_NET(frame, 0.5, device, 'MTCNN/weights/pnet_Weights',
                                  'MTCNN/weights/rnet_Weights', 'MTCNN/weights/onet_Weights')
        faces = Face_alignment(
            frame, default_square=True, landmarks=landmarks)

        embs = []

        test_transform = trans.Compose([
            trans.ToTensor(),
            trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

        for img in faces:
            embs.append(detect_model(
                test_transform(img).to(device).unsqueeze(0)))

        if embs != []:
            source_embs = torch.cat(embs)
            diff = source_embs.unsqueeze(-1) - \
                targets.transpose(1, 0).unsqueeze(0)
            dist = torch.sum(torch.pow(diff, 2), dim=1)
            minimum, min_idx = torch.min(dist, dim=1)
            min_idx[minimum > ((75-156)/(-80))] = -1
            results = min_idx

            for i, k in enumerate(bboxes):
                if results[i] == -1:
                    continue
                student_list.append(names[results[i] + 1])
        return student_list
    except:
        return []
```

- input

```json
ip = http://xxx.xxx.xxx.xxx:xxxx/shot.jpg # ip camera의 영상 주소
```

- ouput

```json
{"id":["4","2","1","0"]} # 인식한 학생의 id 값
```

- 실행화면

<img src="https://github.com/HwangToeMat/HwangToeMat.github.io/blob/master/AI-Project/image/KHUFACE/2/img3.png?raw=true" style="max-width:100%;margin-left: auto; margin-right: auto; display: block;">
